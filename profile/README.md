# [AI Security Research Lab at Miami University](https://miamioh.edu/profiles/cec/samer-khamaiseh.html)

## üëã **Welcome to the Laboratory of AI Security Research (LAiSR) **

## üé§ Who we are? 
Located at Miami University and led by Dr. Samer Khamaiseh. **LAiSR** research group is passionate about exploring and addressing the evolving challenges within the realm of AI security. Our AI Research Laboratory is at the forefront of cutting-edge research to fortify AI models against adversarial attacks, enhance their robustness, and ensure their reliability in real-world scenarios.

## ‚ùì Why **AI security**?
- Emerging Threats: As AI systems become more pervasive, they introduce new vulnerabilities such as adversarial attacks and privacy breaches. Our Research group plays a pivotal role in uncovering vulnerabilities and devising robust defenses.
- Safeguarding Critical Systems: AI is increasingly integrated into critical infrastructure, healthcare, finance, and defense. Ensuring the security of these systems is non-negotiable. Rigorous research helps prevent catastrophic failures and protects lives and livelihoods.
- Ethical Implications: AI decisions impact individuals and societies. Bias, fairness, and transparency are ethical concerns. Research informs guidelines and policies that promote responsible AI deployment, minimizing harm and maximizing benefits.


## üîé Research Focus
- **Adversarial Attacks:** Our goal is to explore new adversarial attacks against AI models. The LAiSR group has introduced *3* novel adversarial attacks such as Target-X, Fool-X, and T2I-Nightmare
- **Adversarial Training:** Exploring defense methods against adversarial attacks. Recently, LAiSR groups introduced the " **VA: Various Attacks Framework for Robust Adversarial Training** & **ADT++: Advanced Adversarial Distributional Training with Class Robustness**" adversarial training methods that promots AI models clean accuracy, roboust accuracy, and robusteness generalization more that the baseline defense methods . 
- **GEN-AI:** Investigate SOTA methods to protect user images from being misused by diffusion models. For example, the LAiSR group proposed **ImagePatroit(Under_Review)** that prevents diffusion models from maliciously adjusting images.
-**GEN-AI Robustness**: Explore the pre and post-generation filters to protect Diffusion models from generating Not-Safe-for-Work (NSFW) contents. **T2I-Vanguard: Post Generation Filter for Safe Text-2-Image Diffusion Models Generation** that prevents T2I attacks from compromising the generation process of T2I diffusion models to produce NSFW images.


## üöÄ Projects
### **Fool-X**
[Add descreption]
### **Image Patriot**
[Add descreption]
### **target-X**
[Add descreption]
### **ADT++**
A novel method for adversarial training.
### **VariousAttacks**
A novel method for adversarial training.

# üì∏ Gallery
<p float="left">
  <img src="https://github.com/user-attachments/assets/4ba8d1d0-b732-4747-b661-1c281e240ff6" width="224" height="300" />
</p>

# üë• Our Team
- [**Dr. Samer Khamaiseh** - Director of LAiSR Research Group](https://www.linkedin.com/in/samer-khamaiseh/)
- [**Deirdre Jost** - Research Assistant](https://www.linkedin.com/in/deirdre-jost-445822228/)
- [**Steven Chiacchira** - Research Assistant](https://www.linkedin.com/in/steven-chiacchira)
- [**Aibak Aljadayah** - Research Assistant](https://www.linkedin.com/in/aibak-aljadayah)
- [**Azib Farooq** - Research Assistant](https://www.linkedin.com/in/itsazibfarooq/)


## üì´ Reach us 
This GitHub account serves as a hub for our ongoing projects, publications, and collaborations. We welcome your engagement and encourage you to explore the exciting frontiers of AI security with us!
[Contact us here](https://miamioh.edu/profiles/cec/samer-khamaiseh.html)





