# [AI Security Research Lab at Miami University](https://miamioh.edu/profiles/cec/samer-khamaiseh.html)

## üëã Welcome to the Laboratory of AI Security Research (LAiSR)

## üé§ Who are we? 
Located at Miami University and led by Dr. Samer Khamaiseh. **LAiSR** research group is passionate about exploring and addressing the evolving challenges within the realm of AI security. Our AI Research Laboratory is at the forefront of cutting-edge research to fortify AI models against adversarial attacks, enhance their robustness, and ensure their reliability in real-world scenarios.

## ‚ùì Why AI security?
- Emerging Threats: As AI systems become more pervasive, they introduce new vulnerabilities through adversarial attacks such as privacy breaches. Our research group plays a pivotal role in uncovering vulnerabilities and devising robust defenses.
- Safeguarding Critical Systems: AI is increasingly integrated into critical infrastructure, healthcare, finance, and defense. Ensuring the security of these systems is non-negotiable. Rigorous research helps prevent catastrophic failures and protects lives and livelihoods.
- Ethical Implications: AI decisions impact individuals and societies. Bias, fairness, and transparency are therefore of ethical concern. Research informs guidelines and policies that promote responsible AI deployment, minimizing harm and maximizing benefits.

## üîé Research Focus
- **Adversarial Attacks:** we explore new adversarial attacks against AI models. The LAiSR group has introduced *3* novel adversarial attacks, including Target-X, Fool-X, and T2I-Nightmare.
- **Adversarial Training:** we explore defense methods against adversarial attacks. Recently, LAiSR introduced the  **VA: Various Attacks Framework for Robust Adversarial Training** and **ADT++: Advanced Adversarial Distributional Training with Class Robustness** adversarial training methods which promote clean accuracy, roboust accuracy, and robusteness generalization more that the baseline defense methods. 
- **GEN-AI:** we investigate SOTA methods to protect user images from being edited by diffusion models. For example, the LAiSR group proposed **ImagePatroit(Under_Review)** that prevents diffusion models from maliciously adjusting images.
-**GEN-AI Robustness**: we explore pre and post-generation filters to prevent Diffusion models from generating Not-Safe-for-Work (NSFW) content with **T2I-Vanguard: Post Generation Filter for Safe Text-2-Image Diffusion Models Generation**.


## üöÄ Research Projects
Below, we list some of the published and ongoing research project at LAiSR lab.

## **[AI Robustness Testing Kit (AiR-TK)](https://github.com/LAiSR-SK/AiRobustnessTestingKit-AiR-TK-)**
AI Robustness Testing Kit (AiR-TK) is an AI testing framework built on PyTorch that enables the AI security community to evaluate existing AI image recognition models against adversarial attacks easily and comprehensively. Air-TK supports adversarial training, the de-facto technique to improve the robustness of AI models against adversarial attacks. Having easy access to state-of-the-art adversarial attacks and the baseline adversarial training method in one place will help the AI security community to replicate, re-use, and improve the upcoming attacks and defense methods

## **[ImagePatriot: Few Pixels is Enough to Protect Images from Generative Models](https://github.com/LAiSR-SK/ImagePatriot)**
By adding magic to a few pixels, ImagePatriot protects your image from being manipulated by diffusion models. 

## **[T2I-Vanguard: Post Generation Filter for Safe Text-2-Image Diffusion Models Generation](https://github.com/LAiSR-SK/T2IVanguard)**

JPA, NightShade, and MMA are recent attacks against Text-2-Image diffusion models which generate Not Safe for Work(NSFW) images despite the pre/post filters. T2I-Vanguard is an ongoing project that aims to provide a shield for T2I models from being compromised by such attacks.

## **[Fool 'Em All --- Fool-X: A Powerful & Fast Method for Generating Effective Adversarial Images](https://github.com/LAiSR-SK/fool-x)**
Fool-X, an algorithm to generate **effective adversarial examples** with the smallest perturbations is able to fool state-of-the-art image classification neural networks. More details are avaiable on project site. (under-Review of IEEE-BigData 2024)

## **[Target-X: An Efficient Algorithm for Generating Targeted Adversarial Images to Fool Neural Networks](https://github.com/LAiSR-SK/target-x)**
Target-X, a novel and fast method for constructing adversarial targeted images on large-scale datasets that can fool the state-of-the-art image classification neural networks. More info available on the project site.

## **[ADT++: Advanced Adversarial Distributional Training for AI Robustness](https://github.com/LAiSR-SK/ADT2Plus)**
ADT++ is a fast adversarial training method for AI models which increases their generalization robustness against more adaptive adversarial attacks such as Target-X and AutoAttack (AA).

## **[VA: Various Attacks Framework for Robust Adversarial Training](https://github.com/LAiSR-SK/VariousAttacks)**
VA can be used to increase the robustness of AI models against variety of adversarial gradient-based attacks by exploring class robustness.

# ** Introduction to AI Security
Interested researchers and scientist can refer to the following resources to start their journey in AI security.
- [Adversarial Deep Learning: A Survey on Adversarial Attacks and Defense Mechanisms on Image Classification](https://ieeexplore.ieee.org/abstract/document/9895425)
- [Target-X: An Efficient Algorithm for Generating Targeted Adversarial Images to Fool Neural Networks](https://ieeexplore.ieee.org/document/10197071)
- [Adversarial Robustness - Theory and Practice](https://adversarial-ml-tutorial.org/)


# üë• Our Team
- [**Dr. Samer Khamaiseh** - Director of LAiSR Research Group](https://www.linkedin.com/in/samer-khamaiseh/)
- [**Deirdre Jost** - Research Assistant](https://www.linkedin.com/in/deirdre-jost-445822228/)
- [**Steven Chiacchira** - Research Assistant](https://www.linkedin.com/in/steven-chiacchira)
- [**Aibak Aljadayah** - Research Assistant](https://www.linkedin.com/in/aibak-aljadayah)
- [**Azib Farooq** - Research Assistant](https://www.linkedin.com/in/itsazibfarooq/)

## üì´ Reach us 
This GitHub organization serves as a hub for our ongoing projects, publications, and collaborations. We welcome your engagement and encourage you to explore the exciting frontiers of AI security with us!
[Contact us here](khamaisy@miamioh.edu)

